[package]
name = "rusty-commit"
version = "1.0.27"
edition = "2021"
authors = ["Rusty Commit Contributors"]
description = "Rust-powered AI commit message generator - Write impressive commits in seconds"
license = "MIT"
repository = "https://github.com/hongkongkiwi/rusty-commit"
keywords = ["git", "ai", "commit", "rust", "cli"]
categories = ["command-line-utilities", "development-tools"]
readme = "README.md"
homepage = "https://github.com/hongkongkiwi/rusty-commit"
documentation = "https://docs.rs/rusty-commit"

# Publish to crates.io
publish = ["crates-io"]

[[bin]]
name = "rco"
path = "src/main.rs"

[features]
# ═══════════════════════════════════════════════════════════════════════════════
# Default Features - Common providers enabled by default for convenience
# Users can disable individual providers to reduce binary size
# ═══════════════════════════════════════════════════════════════════════════════
default = [
    # Core providers
    "openai",
    "anthropic",
    "ollama",
    "gemini",
    "azure",
    "perplexity",
    "xai",
    # Major OpenAI-compatible providers
    "openrouter",
    "groq",
    "deepseek",
    "together",
    "deepinfra",
    "mistral",
    "github-models",
    "fireworks",
    "moonshot",
    "dashscope",
    # Enterprise providers
    "cohere",
    "ai21",
    "upstage",
    # Popular GPU/Inference providers
    "cerebras",
    "sambanova",
    "novita",
    "nebius",
    "replicate",
    "lambda",
    "hyperbolic",
    "kluster",
    # Local providers
    "lmstudio",
    "llamacpp",
    # China-based providers
    "siliconflow",
    "zhipu",
    "minimax",
    # AI Gateways
    "helicone",
    # Cloud providers
    "huggingface",
    "bedrock",
    "vertex",
    # Dedicated providers
    "mlx",
    "nvidia",
    "flowise",
    # Additional popular providers
    "jina",
    "poe",
    "venice",
]
# Enable secure storage for API keys (requires system keychain)
secure-storage = ["keyring"]
# Include extra documentation (for docs.rs)
docs = []

# ═══════════════════════════════════════════════════════════════════════════════
# Provider Feature Groups - Enable all providers in a category
# ═══════════════════════════════════════════════════════════════════════════════
# Popular cloud providers (OpenAI, Anthropic, etc.)
providers-popular = ["openai", "anthropic", "ollama", "gemini", "xai", "azure"]
# Fast inference providers (Groq, Cerebras, SambaNova)
providers-fast = ["groq", "cerebras", "sambanova", "nebius"]
# OpenAI-compatible providers (OpenRouter, DeepSeek, Together, etc.)
providers-openai-compatible = ["openrouter", "deepseek", "together", "deepinfra", "mistral", "fireworks", "moonshot", "dashscope", "cohere", "ai21", "upstage", "replicate", "lambda", "hyperbolic", "kluster", "venice", "jina", "poe", "novita", "friendli", "baseten"]
# Local/self-hosted providers
providers-local = ["lmstudio", "llamacpp", "ollama"]
# Enterprise providers
providers-enterprise = ["bedrock", "vertex", "azure"]
# China-based providers
providers-china = ["siliconflow", "zhipu", "minimax", "baichuan"]
# GPU cloud providers
providers-gpu-cloud = ["nebius", "lambda", "hyperbolic", "scaleway", "ovh"]
# All providers (for maximum functionality, larger binary)
providers-all = ["providers-popular", "providers-fast", "providers-openai-compatible", "providers-local", "providers-enterprise", "providers-china", "providers-gpu-cloud", "huggingface", "mlx", "nvidia", "flowise"]

# ═══════════════════════════════════════════════════════════════════════════════
# Individual Provider Features
# ═══════════════════════════════════════════════════════════════════════════════
openai = ["async-openai"]
anthropic = []
ollama = []
gemini = []
azure = []
perplexity = []
xai = ["async-openai"]
huggingface = []
bedrock = []
vertex = ["chrono"]

# ═══════════════════════════════════════════════════════════════════════════════
# OpenAI-Compatible Provider Presets
# These don't need extra dependencies, they use the openai module
# ═══════════════════════════════════════════════════════════════════════════════

# Major Cloud Providers
openrouter = ["openai"]
groq = ["openai"]
deepseek = ["openai"]
together = ["openai"]
deepinfra = ["openai"]
mistral = ["openai"]
github-models = ["openai"]
fireworks = ["openai"]
moonshot = ["openai"]
dashscope = ["openai"]

# Enterprise & Specialized
cohere = ["openai"]
ai21 = ["openai"]
upstage = ["openai"]

# GPU Cloud & Inference Providers (Comprehensive list from OpenCode)
nebius = ["openai"]
ovh = ["openai"]
scaleway = ["openai"]
friendli = ["openai"]
baseten = ["openai"]
chutes = ["openai"]
ionet = ["openai"]
modelscope = ["openai"]
requesty = ["openai"]
morph = ["openai"]
synthetic = ["openai"]
nano-gpt = ["openai"]
zenmux = ["openai"]
v0 = ["openai"]
venice = ["openai"]
cortecs = ["openai"]
abacus = ["openai"]
bailing = ["openai"]
fastrouter = ["openai"]
inference = ["openai"]
submodel = ["openai"]
zai = ["openai"]
poe = ["openai"]
cerebras = ["openai"]
sambanova = ["openai"]
novita = ["openai"]
predibase = ["openai"]
tensorops = ["openai"]
hyperbolic = ["openai"]
kluster = ["openai"]
lambda = ["openai"]
replicate = ["openai"]
targon = ["openai"]
corcel = ["openai"]
cybernative = ["openai"]
edgen = ["openai"]
gigachat = ["openai"]
hydra = ["openai"]
jina = ["openai"]
lingyi = ["openai"]
monica = ["openai"]
pollinations = ["openai"]
shuttleai = ["openai"]
teknium = ["openai"]
theb = ["openai"]
tryleap = ["openai"]

# Local/Self-hosted Providers
lmstudio = ["openai"]
llamacpp = ["openai"]
kobold = ["openai"]
textgen = ["openai"]
tabby = ["openai"]

# China-based Providers
siliconflow = ["openai"]
zhipu = ["openai"]
minimax = ["openai"]
baichuan = ["openai"]

# AI Gateway & Proxy Services
helicone = ["openai"]
workers-ai = ["openai"]
cloudflare-ai = ["openai"]
vercel-ai = ["openai"]

# Specialized Providers
302ai = ["openai"]
sap-ai = ["openai"]

# Additional Providers from OpenCommit
aimlapi = ["openai"]

# ═══════════════════════════════════════════════════════════════════════════════
# Dedicated Providers with Special Implementations
# ═══════════════════════════════════════════════════════════════════════════════
mlx = []
nvidia = []
flowise = []

[dependencies]
# CLI and UI
clap = { version = "4.5", features = ["derive", "env", "help"] }
clap_complete = "4.5"
clap_complete_fig = "4.5"
colored = "3.0"
dialoguer = { version = "0.12", features = ["fuzzy-select"] }
indicatif = "0.18"
console = "0.16"

# Async runtime
tokio = { version = "1.35", features = ["full"] }
async-trait = "0.1"

# HTTP and API clients
reqwest = { version = "0.12", default-features = false, features = ["json", "stream", "rustls-tls", "blocking"] }
async-openai = { version = "0.32", optional = true, features = ["chat-completion"] }
backoff = { version = "0.4", features = ["tokio"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
toml = "0.9"

# Git operations
git2 = "0.20"

# Error handling
anyhow = "1.0"
thiserror = "2.0"

# Configuration
config = "0.15"
dirs = "6.0"

# Utilities
regex = "1.10"
once_cell = "1.19"
url = "2.5"
base64 = "0.22"
chrono = { version = "0.4", default-features = false, optional = true }

# Cloud providers
aws-config = { version = "1", default-features = false }
aws-sdk-bedrockruntime = { version = "1", default-features = false }
sha2 = "0.10"
tiktoken-rs = "0.9"
keyring = { version = "3.6", optional = true }
uuid = { version = "1.10", features = ["v4"] }
webbrowser = "1.0"
warp = { version = "0.4", features = ["server"] }
semver = "1.0"
which = "8.0"
tempfile = "3.9"
futures = "0.3"
flate2 = "1.0"
tar = "0.4"
zip = "7.1"
arboard = "3.4"

# MCP (Model Context Protocol) support
rmcp = { version = "0.13.0", features = ["server"] }
serde_urlencoded = "0.7"
rand = "0.9"

# OpenSSL for static musl/Alpine builds
openssl = { version = "0.10", features = ["vendored"] }

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Testing
[dev-dependencies]
mockito = "1.2"
assert_cmd = "2.0"
predicates = "3.0"
proptest = "1.4"

[profile.release]
lto = true
opt-level = 3
codegen-units = 1
strip = true
